{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Effects Classifier Example\n",
    "\n",
    "To demonstrate how to use the [IDMT-SMT-Audio Effects](https://mirdata.readthedocs.io/en/stable/source/mirdata.html#module-mirdata.datasets.idmt_smt_audio_effects) data loader available in mirdata, this Jupyter notebook goes through the process of training a simple audio effects classifier using the [PyTorch](https://pytorch.org/) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francescopapaleo/mirdata-notebooks/blob/master/idmt_smt_audio_effects/audio_effects_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.8\n"
     ]
    }
   ],
   "source": [
    "#@title Imports\n",
    "import mirdata\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(mirdata.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "def print_directory_tree(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, \"\").count(os.sep)\n",
    "        indent = \" \" * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = \" \" * 4 * (level + 1)\n",
    "        # Using a list comprehension to filter out .wav and .xml files\n",
    "        files_to_print = [f for f in files if not f.endswith((\".wav\", \".xml\", \".\"))]\n",
    "        for f in files_to_print:\n",
    "            print(f\"{subindent}{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/francescopapaleo/mir_datasets/idmt_smt_audio_effects\n"
     ]
    }
   ],
   "source": [
    "#@title Initialize the dataset\n",
    "idmt_smt_audio_effects = mirdata.initialize(\"idmt_smt_audio_effects\")\n",
    "print(idmt_smt_audio_effects.data_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download the dataset\n",
    "# Uncomment the line below to download the dataset\n",
    "# idmt_smt_audio_effects.download(force_overwrite=False, cleanup=True)\n",
    "\n",
    "# Print the directory tree\n",
    "# print_directory_tree(idmt_smt_audio_effects.data_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [00:00<00:00, 1212.01it/s]\n",
      "100%|██████████| 55044/55044 [01:44<00:00, 527.16it/s]\n",
      "INFO: Success: the dataset is complete and all files are valid.\n",
      "INFO: --------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'metadata': {}, 'tracks': {}}, {'metadata': {}, 'tracks': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Validate the dataset\n",
    "idmt_smt_audio_effects.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class IDMTSMTAudioEffectsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for the IDMT-SMT-AUDIO-EFFECTS dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_home (str): The path to the folder where the dataset is stored.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dataset = mirdata.initialize(\"idmt_smt_audio_effects\")\n",
    "        self.transform = transform\n",
    "        self.tracks = self.dataset.load_tracks()  # Load all track metadata\n",
    "\n",
    "        self.monophonic_tracks = {\n",
    "            k: v for k, v in self.tracks.items() if 'polyphon' not in os.path.split(v.audio_path)[0]\n",
    "        }\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the item to fetch.\n",
    "\n",
    "        Returns:\n",
    "            (Tensor, int): Tuple containing (audio waveform as a tensor, label as an integer)\n",
    "        \"\"\"\n",
    "        track_id = list(self.monophonic_tracks.keys())[idx]\n",
    "        track = self.monophonic_tracks[track_id]\n",
    "\n",
    "        # Load the audio file\n",
    "        audio, sr = track.audio\n",
    "\n",
    "        # Apply preprocessing if needed\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        # Convert the audio array to a PyTorch tensor\n",
    "        audio_tensor = torch.tensor(audio, dtype=torch.float32)\n",
    "\n",
    "        label_map = {\n",
    "            11: 0,  # No effect\n",
    "            12: 1,  # No Effect, amplifier simulation\n",
    "            21: 2,  # Feedback Delay\n",
    "            22: 3,  # Slapback Delay\n",
    "            23: 4,  # Reverb\n",
    "            31: 5,  # Chorus\n",
    "            32: 6,  # Flanger\n",
    "            33: 7,  # Phaser\n",
    "            34: 8,  # Tremolo\n",
    "            35: 9,  # Vibrato\n",
    "            41: 10,  # Distortion\n",
    "            42: 11,  # Overdrive\n",
    "        }\n",
    "\n",
    "        label = label_map.get(\n",
    "            track.fx_type, -1\n",
    "        )  # Default to -1 if fx_type is not found\n",
    "\n",
    "        return audio_tensor, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: The size of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.monophonic_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41184\n",
      "torch.Size([88201])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "dataset = IDMTSMTAudioEffectsDataset()\n",
    "print(len(dataset))  # Prints the number of items in the dataset\n",
    "audio, label = dataset[0]  # Gets the first sample\n",
    "print(audio.shape)  # Prints the shape of the audio tensor\n",
    "print(label)  # Prints the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioEffectClassifier(nn.Module):\n",
    "    def __init__(self, audio_length, num_classes=12):\n",
    "        super(AudioEffectClassifier, self).__init__()\n",
    "\n",
    "        # Define the layers of the network\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the flattened features after the conv and pool layers\n",
    "        feature_size = self._calculate_feature_size(audio_length)\n",
    "        self.fc1 = nn.Linear(feature_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _calculate_feature_size(self, audio_length):\n",
    "        # Simulate the forward pass for the convolutional and pooling layers to calculate the feature size\n",
    "        x = torch.rand(1, 1, audio_length)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x.numel()\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        # This method is not needed if _calculate_feature_size is used\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "audio_length = 88201  # The length of the audio samples in the dataset\n",
    "model = AudioEffectClassifier(audio_length)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Adam is a good default choice)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv1d: 1, MaxPool1d: 1, Conv1d: 1, MaxPool1d: 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/mirdata-env/lib/python3.10/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49mx, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/mirdata-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/mirdata-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "\u001b[1;32m/Users/francescopapaleo/src-git/mirdata-notebooks/idmt_smt_audio_effects/audio_effects_classifier.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescopapaleo/src-git/mirdata-notebooks/idmt_smt_audio_effects/audio_effects_classifier.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/francescopapaleo/src-git/mirdata-notebooks/idmt_smt_audio_effects/audio_effects_classifier.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_flat_features(x))  \u001b[39m# Flatten the tensor for the fully connected layer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescopapaleo/src-git/mirdata-notebooks/idmt_smt_audio_effects/audio_effects_classifier.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n",
      "\u001b[0;31mTypeError\u001b[0m: view(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got NoneType\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/francescopapaleo/src-git/mirdata-notebooks/idmt_smt_audio_effects/audio_effects_classifier.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/francescopapaleo/src-git/mirdata-notebooks/idmt_smt_audio_effects/audio_effects_classifier.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m88200\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/francescopapaleo/src-git/mirdata-notebooks/idmt_smt_audio_effects/audio_effects_classifier.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Print the summary of the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/francescopapaleo/src-git/mirdata-notebooks/idmt_smt_audio_effects/audio_effects_classifier.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m summary(model, input_size\u001b[39m=\u001b[39;49mdummy_input)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/mirdata-env/lib/python3.10/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[39m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[39m=\u001b[39m forward_pass(\n\u001b[1;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m formatting \u001b[39m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[39m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/mirdata-env/lib/python3.10/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[39m=\u001b[39m [layer \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m summary_list \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecuted layers up to: \u001b[39m\u001b[39m{\u001b[39;00mexecuted_layers\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv1d: 1, MaxPool1d: 1, Conv1d: 1, MaxPool1d: 1]"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Assuming `AudioEffectClassifier` is your model class and `audio_length` is defined\n",
    "model = AudioEffectClassifier(audio_length=audio_length, num_classes=12)\n",
    "\n",
    "# Replace `dummy_input` with the actual size of the input data\n",
    "# For example, if your input data is 1 channel, 88200 samples long:\n",
    "dummy_input = (1, 1, 88200)\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, input_size=dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader setup\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        # Transfer Data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirdata-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
